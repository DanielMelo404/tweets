{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_20820\\2750072062.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5329, 2284, 5329, 2284)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/train.csv')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.text,df.target,test_size=0.3,random_state=42, shuffle=False)\n",
    "X_train.shape[0], X_test.shape[0], y_train.shape[0], y_test.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_encoded_df= pd.read_csv('data/one_hot_encoded_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['!', '!!', '!!!', '!!!!', '!!!!!', '!!!!!!!!!!!#MetroFmTalk', '!The',\n",
      "       '#', '##book', '##fukushima',\n",
      "       ...\n",
      "       'å£9!', 'å¤}', 'å¨', 'å©Daniel', 'å¬'Only', 'åÇ', 'åÈ',\n",
      "       'åÈMGN-AFRICAå¨', 'åÊ', 'åÊFedEx'],\n",
      "      dtype='object', length=31924)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame containing the tweets\n",
    "# Create the vocabulary\n",
    "\n",
    "vocabulary = np.array([])\n",
    "for i in range(len(df.index)):\n",
    "    words = df['text'].iloc[i].split()  \n",
    "    vocabulary = np.append(vocabulary, np.unique(words))  \n",
    "    vocabulary = np.unique(vocabulary)\n",
    "\n",
    "# Create a dictionary to map each word to an index\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "# Function to one-hot encode a tweet\n",
    "def one_hot_encode(tweet, vocab_size):\n",
    "    one_hot_vector = np.zeros(vocab_size)\n",
    "    for word in tweet.split():\n",
    "        if word in word_to_index:\n",
    "            index = word_to_index[word]\n",
    "            one_hot_vector[index] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "# Apply the one-hot encoding to each tweet\n",
    "one_hot_encoded_tweets = np.array([one_hot_encode(tweet, len(vocabulary)) for tweet in df['text']])\n",
    "\n",
    "# Convert to DataFrame for better readability (optional)\n",
    "one_hot_encoded_df = pd.DataFrame(one_hot_encoded_tweets, columns=vocabulary)\n",
    "print(one_hot_encoded_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31924"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_letters = one_hot_encoded_df.shape[1]\n",
    "n_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterToIndex(letter):\n",
    "    return one_hot_encoded_df.columns.get_loc(letter)\n",
    "\n",
    "def lineToTensor(line):\n",
    "    line = line.split()\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # all_categories.index\n",
    "        hidden = F.tanh(self.i2h(input) + self.h2h(hidden))\n",
    "        output = self.h2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "all_categories = [1,0]\n",
    "\n",
    "rnn = torch.load('rnn1.pth')\n",
    "\n",
    "n_correct = 0\n",
    "for index, row in y_test.items():\n",
    "    line_tensor = lineToTensor(X_test.loc[index])\n",
    "    hidden = rnn.initHidden()\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        category = torch.tensor([all_categories.index(y_test.iloc[i])], dtype=torch.long)\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    if guess == category:\n",
    "        n_correct +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5030647985989493"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = n_correct/y_test.shape[0]\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_categories = [1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 2848,\n",
       " tensor([0]),\n",
       " tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(df[df.target==category].index)\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(df.text.iloc[line])\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "randomChoice(all_categories)\n",
    "\n",
    "randomChoice(df[df.target==1].index)\n",
    "\n",
    "randomTrainingExample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openbox3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
